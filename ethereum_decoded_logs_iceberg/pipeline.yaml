Engine:
  Local:
    Bundles:
      - https://github.com/agnosticeng/agnostic-clickhouse-udf/releases/download/v0.0.14/agnostic-clickhouse-udf_0.0.14_{{ .PLATFORM | default "linux_amd64_v3" }}.tar.gz
      - https://github.com/agnosticeng/icepq/releases/download/v0.0.5/icepq_clickhouse_udf_bundle_0.0.5_{{ .PLATFORM | default "linux_amd64_v3" }}.tar.gz
    Settings:
      allow_experimental_dynamic_type: 1
      enable_named_columns_in_function_tuple: 1
      send_logs_level: warning
      enable_json_type: 1
      enable_dynamic_type: 1
      schema_inference_make_columns_nullable: auto
      remote_filesystem_read_prefetch: 0
      input_format_parquet_use_native_reader: 1
      output_format_parquet_string_as_string: 0
      output_format_parquet_use_custom_encoder: 1
      output_format_parquet_write_page_index: 1
      output_format_parquet_write_bloom_filter: 1
      output_format_parquet_parallel_encoding: 0
      max_execution_time: 3600
      max_bytes_ratio_before_external_group_by: 0.01
      max_bytes_before_external_group_by: 128MB
      max_bytes_ratio_before_external_sort: 0.01
      max_bytes_before_external_sort: 128MB
      s3_max_connections: 50

Init:
  Queries:
    - init_start.sql
  ClickhouseSettings:
    max_threads: 4
    max_download_buffer_size: 1048576
    
Source:
  Query: source.sql
  StopOnEmpty: true

Processors:
  - Map: 
      Queries:
        - create_buffer.sql
    Workers: 1
    ClickhouseSettings:
      max_threads: 2
  - Seq: {}
  - Batch:
      Queries:
        - merge_buffers.sql
      SizeQuery: buffers_size.sql
      MaxWait: 120s
      MaxSize: 100000000
  - Map: 
      Queries:
        - write_parquet_file.sql
        - append_to_table.sql
        - delete_buffers.sql
      ClickhouseSettings:
        max_threads: 1
        # s3_allow_parallel_part_upload=0,
        # s3_max_inflight_parts_for_one_file=1,
        # s3_max_upload_part_size=67108864,
        # max_block_size=32768,
        # min_insert_block_size_rows=0,
        # min_insert_block_size_bytes=33554432